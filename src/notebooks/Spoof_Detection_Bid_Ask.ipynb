{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Complete make_one_shot_task\n",
    "# 2. Complete test one shot task\n",
    "# 3. Complete plotting\n",
    "# 4. See if using attention with some static probabilities help the detection of spoofing\n",
    "# 5. Look at triple loss and computing it\n",
    "# 6. CNN and attention to top of the book - https://towardsdatascience.com/self-attention-in-computer-vision-2782727021f6\n",
    "    #Propose probabilities learnt through an LSTM network \n",
    "# 7. Higher prediction probabilities for top of book\n",
    "    #https://machinelearningmastery.com/how-to-score-probability-predictions-in-python/\n",
    "# 8. Fuck with data to get the desired result -b rebuild project\n",
    "# 9. Clean up code, make fancy plots, write comments and so on\n",
    "# 10. See if using LSTM to learn probabilities for attention would work \n",
    "# 11. Learn Families of classes for spoofing \n",
    "# 12. Custom loss function for family of identifications\n",
    "# 13. Cross Modal for price and qty - https://iclr.cc/virtual/poster_B1lJzyStvS.html\n",
    "# 14. Mutual information for either layering or other task - https://iclr.cc/virtual/poster_rkxoh24FPH.html, https://openreview.net/forum?id=rkxoh24FPH\n",
    "# 15. Time seires with triplet leraning - https://papers.nips.cc/paper/8713-unsupervised-scalable-representation-learning-for-multivariate-time-series.pdf\n",
    "# 16. Quadraple Learning - https://www.youtube.com/watch?v=_o2SLgjejAE\n",
    "# 17. Add 13/14/15/16 for the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO to make this PERFECT\n",
    "# 1. Normalise all data - DONE\n",
    "# 2. Use prices as well - Explore various convolutions whether depth wise or 3d and so on - https://keras.io/layers/convolutional/ - DONE\n",
    "# 3. Bias / Weights inititalisation\n",
    "# 4. Get better data generating process - DONE\n",
    "# 5. Use the new data for better prediction. Some changes to Y variable and so on. \n",
    "# 6. Do spoof buy and sell labels\n",
    "# 6. Create better images of the classes\n",
    "# 7. Get proper error rate\n",
    "# 8. Improve the network\n",
    "!pip install keras-tcn==2.8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RESULTS\n",
    "1. No-Change Unnormalised Loss : 0.012634731829166412\n",
    "2. Normalised Loss without anchor normalisation : 0.5267388820648193\n",
    "3. Normalised Loss with anchor : 0.20129983127117157"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "project_path = os.path.abspath(os.path.join('../..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "if project_path not in sys.path:\n",
    "    sys.path.append(project_path)\n",
    "\n",
    "import config\n",
    "import spoof_ground_truth\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from imageio import imread\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pylab import *\n",
    "import time\n",
    "import numpy.random as rng\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv1D,Conv2D, ZeroPadding2D, Activation, concatenate, Dropout, Input\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling1D, MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.utils import plot_model,normalize\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer, StandardScaler, OneHotEncoder\n",
    "from skimage.util.shape import view_as_windows\n",
    "from tcn import TCN\n",
    "from tensorflow_addons.losses import triplet_semihard_loss\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "import keras\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 56} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_iteration=0\n",
    "nb_classes=2\n",
    "cols, rows = 2, 30\n",
    "input_shape = (cols, rows, 1)\n",
    "\n",
    "# Hyper parameters\n",
    "evaluate_every = 500 # interval for evaluating on one-shot tasks\n",
    "batch_size = 32\n",
    "n_iter = 2000 # No. of training iterations\n",
    "n_val = 250 # how many one-shot tasks to validate on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "abeo_train = np.load(project_path + '/data/train/20160901/ABEO_BATS.npy')\n",
    "abeo_val = np.load(project_path + '/data/train/20160901/ABEO_NASDAQ.npy')\n",
    "\n",
    "\n",
    "#goog_train_bid = np.load(project_path + '/data/train/bid/20160901/GOOG_BATS.npy')\n",
    "#goog_train_ask = np.load(project_path + '/data/train/ask/20160901/GOOG_BATS.npy')\n",
    "\n",
    "#goog_val_bid = np.load(project_path + '/data/train/bid/GOOG_NASDAQ.npy')\n",
    "#goog_val_ask = np.load(project_path + '/data/train/ask/GOOG_NASDAQ.npy')\n",
    "\n",
    "model_path = project_path + '/weights/spoof'\n",
    "model_cp_path = project_path + '/check_point/spoof'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 30, 2)\n"
     ]
    }
   ],
   "source": [
    "min_max_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "qq = QuantileTransformer()\n",
    "data = np.array([9.7, 9.5, 9.3, 8.1])\n",
    "data_r = data.reshape(-1, 1)\n",
    "x = min_max_scaler.fit_transform(data_r)\n",
    "y = scaler.fit_transform(data_r)\n",
    "z = qq.fit_transform(data_r)\n",
    "#print(x)\n",
    "#print(y)\n",
    "#print(z)\n",
    "\n",
    "#print(spoof_ground_truth.spoof_step1_truth1[:,1])\n",
    "a = spoof_ground_truth.spoof_step1_truth1[:,0].reshape(2, 30, 1)\n",
    "b = spoof_ground_truth.spoof_step1_truth1[:,1].reshape(2, 30, 1)\n",
    "c = np.dstack((a,b))\n",
    "print(c.shape)\n",
    "#print(spoof_ground_truth.spoof_type_one.reshape(2,10,3,11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second After Reshape\n",
      "(3243, 200, 30, 2, 2)\n",
      "(3243,)\n",
      "Second After Reshape\n",
      "(1606, 200, 30, 2, 2)\n",
      "(1606,)\n"
     ]
    }
   ],
   "source": [
    "def retrieve_cleansed_data(lob, width=200):\n",
    "    # when you do view below it gives you 0 and yhen evrything after\n",
    "    #   print(lob_states[19])\n",
    "    #  print(Y_labels[19]) - abeo arca test case\n",
    "    min_max_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    quantile_transformer = QuantileTransformer()\n",
    "    one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "     \n",
    "    #print(lob['quantity'][:,0,0,0:15].shape)\n",
    "    #print(lob['quantity'][:,:,0,0:15].shape)\n",
    "    # As evidenced by above, we can technically select all in the second axis as there is only 1 element. However, \n",
    "    # because we need a 2d input we make it 0. The 3rd axis is side so we need this\n",
    "    lob_qty_buy = pd.DataFrame(lob['quantity'][:,0,0,0:15])\n",
    "    max_buy = np.amax(lob_qty_buy, axis=1)\n",
    "    lob_qty_buy = lob_qty_buy.replace(0, np.NaN)\n",
    "    avg_buy = lob_qty_buy.mean().mean()\n",
    "    vol_sum_buy = lob_qty_buy.sum(axis=1)\n",
    "    \n",
    "    lob_qty_sell = pd.DataFrame(lob['quantity'][:,0,1,0:15])\n",
    "    max_sell = np.amax(lob_qty_sell, axis=1)\n",
    "    lob_qty_sell = lob_qty_sell.replace(0, np.NaN)\n",
    "    avg_sell = lob_qty_sell.mean().mean()\n",
    "    vol_sum_sell = lob_qty_sell.sum(axis=1)\n",
    "    \n",
    "    vol_imbalance = (vol_sum_buy - vol_sum_sell) / (vol_sum_buy + vol_sum_sell)\n",
    "    vol_imbalance = (vol_sum_buy - vol_sum_sell) / (vol_sum_buy + vol_sum_sell)\n",
    "    \n",
    "    label_df = pd.concat([vol_imbalance, pd.Series(lob['action'].ravel()), pd.Series(lob['side'].ravel()), \n",
    "                         max_buy, max_sell], axis=1)\n",
    "    label_df[5] = label_df[0].diff() # Change in the values of the two states of OB (We take diff of prev row)\n",
    "    label_df[6] = 0\n",
    "  \n",
    "    label_df[6] = np.where(((label_df[1] == 2) & (label_df[2] == 'B') & (np.abs(label_df[5]) > 0.09) & \n",
    "                            (label_df[3] > avg_buy)), 1, \n",
    "                             np.where(((label_df[1] == 2) & (label_df[2] == 'S') & (np.abs(label_df[5]) > 0.09) &\n",
    "                                       (label_df[3] > avg_sell)), 2, label_df[6].values))\n",
    "    label_df = label_df.iloc[width-1:]\n",
    "    Y_labels = label_df[6].reset_index(drop=True)\n",
    "   \n",
    "    # Normalise positive samples\n",
    "    # these array manipulations are to get a final array where b-s in same array group\n",
    "    lob_n, d, w, h = lob['quantity'].shape\n",
    "    b_qty = lob['quantity'][:,0,0,:]\n",
    "    s_qty = lob['quantity'][:,0,1,:]\n",
    "    lob_qty = np.stack((b_qty, s_qty), axis=2)\n",
    "   # print(lob_qty[6])\n",
    "   # print('Second After Reshape')\n",
    "   # print(lob['quantity'][6])\n",
    "    lob_qty = lob_qty.ravel()\n",
    "   # lob_qty = min_max_scaler.fit_transform(lob_qty)\n",
    "    lob_qty = lob_qty.reshape(lob_n, h, w)\n",
    "   # print(lob_qty[6])\n",
    "    \n",
    "    \n",
    "    b_price = lob['price'][:,0,0,:]\n",
    "    s_price = lob['price'][:,0,1,:]\n",
    "    lob_price = np.stack((b_price, s_price), axis=2)\n",
    "   # print(lob_price[6])\n",
    "    #print('Second After Reshape')\n",
    "    lob_price = lob_price.ravel()\n",
    "    #lob_price = quantile_transformer.fit_transform(lob_price)\n",
    "    lob_price = lob_price.reshape(lob_n, h, w)\n",
    "    #print(lob_price[6])\n",
    "    \n",
    "    triplets = np.zeros((1, 3, 2, 13)) \n",
    " #   print(triplets[0])\n",
    "    #lob_orderid = lob['order_id'].reshape(-1,1)\n",
    "    #lob_orderid = one_hot_encoder.fit_transform(lob_orderid)\n",
    "    #lob_orderid = lob_orderid.reshape(lob_n, h, w, d)\n",
    "\n",
    "    #print(lob_price[6])\n",
    "    lob_states = np.stack((lob_qty, lob_price), axis=-1)\n",
    "   # print(lob_states[6])\n",
    "    lob_states = lob_states.reshape(lob_n, h, w, 2)\n",
    "    print('Second After Reshape')\n",
    "    #print(lob_states[6])\n",
    "    lob_states = view_as_windows(lob_states,(width,1,1,1))[...,0,0,0].transpose(0,4,1,2,3)\n",
    "   # print(lob_states[6])\n",
    "    # TODO : TEST CASE ASSERT WITH WIDTH = 2 and CHECK ALL CORRECT\n",
    "   #\n",
    "   # print(lob_states[19])\n",
    "  #  print(Y_labels[19])\n",
    "    print(lob_states.shape)\n",
    "    print(Y_labels.shape)\n",
    "    return lob_states, Y_labels\n",
    "\n",
    "\n",
    "X_train, Y_train = retrieve_cleansed_data(abeo_val)\n",
    "X_test, Y_test = retrieve_cleansed_data(abeo_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_pos_neg(pos_sample, neg_sample, pos_target=None, neg_target=None):\n",
    "    \"\"\"\n",
    "    Combines positive and negative samples into one sample set and returns a dataset that can be used for train/test\n",
    "    or validation. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pos_sample : ndarray of shape (n_features,)\n",
    "        Positive cleansed and normalised data \n",
    "    neg_sample : ndarray of shape (n_features,)\n",
    "        Negative cleansed and normalised data \n",
    "    pos_target : ndarray of shape (n_features,)\n",
    "        Target classes if exist \n",
    "    neg_target : ndarray of shape (n_features,)\n",
    "        Target classes if exist \n",
    "    \"\"\"\n",
    "        \n",
    "    list_of_samples = []\n",
    "    sample = np.append(pos_sample, neg_sample, axis=0)\n",
    "    target = np.append(np.ones(pos_sample.shape[0]), np.zeros(neg_sample.shape[0]))\n",
    "    sample, target = shuffle(sample, target)\n",
    "    list_of_samples = [pos_sample, neg_sample]\n",
    "    return sample, target, list_of_samples\n",
    "\n",
    "#anchor, pos_test, neg_test = retrieve_cleansed_data(abeo_val_pos, \n",
    "     #                                               abeo_val_neg, spoof_ground_truth.spoof_step1_truth1, 'B')\n",
    "#anchor, pos_val, neg_val = retrieve_cleansed_data(goog_val_pos, goog_val_neg, \n",
    "   #                                               spoof_ground_truth.spoof_step1_truth1, 'B')\n",
    "\n",
    "#x_train, y_train, list_of_train = combine_pos_neg(pos_train, neg_train)\n",
    "#x_test, y_test, list_of_test = combine_pos_neg(pos_test, neg_test)\n",
    "#x_val, y_val, list_of_val = combine_pos_neg(pos_val, neg_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3442, 5)\n",
      "(3440, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_x = np.zeros((3442, 5))\n",
    "print(input_x.shape)\n",
    "input_x = np.delete(input_x, np.s_[:2], axis=0)\n",
    "#out = view_as_windows(input_x,(200,1))[...,0].transpose(0,1,2)\n",
    "print(input_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shape, name=None, dtype=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)\n",
    "\n",
    "def initialize_bias(shape, name=None, dtype=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Plot of weights initialized, with mean of 0.0 and standard deviation of 0.01')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Intialize bias with mean 0.0 and standard deviation of 10^-2\n",
    "weights = initialize_weights((1000,1))\n",
    "sns.distplot(weights)\n",
    "plt.title(\"Plot of weights initialized, with mean of 0.0 and standard deviation of 0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Plot of biases initialized, with mean of 0.0 and standard deviation of 0.01')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Intialize bias with mean 0.5 and standard deviation of 10^-2\n",
    "bias = initialize_bias((1000,1))\n",
    "sns.distplot(bias)\n",
    "plt.title(\"Plot of biases initialized, with mean of 0.0 and standard deviation of 0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape):\n",
    "    \"\"\"\n",
    "        Model architecture\n",
    "    \"\"\"\n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "\n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(1,1), activation='relu', strides=1, padding=\"same\", input_shape=input_shape,\n",
    "                     use_bias = True, kernel_initializer=initialize_weights, bias_initializer=initialize_bias))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), activation='relu', strides=1, padding=\"same\", input_shape=input_shape,\n",
    "                     use_bias = True, kernel_initializer=initialize_weights, bias_initializer=initialize_bias))\n",
    "    # model.add(MaxPooling2D())\n",
    "    #model.add(Conv2D(128,kernel_size=(1,1), activation='relu', strides=1, padding=\"same\", input_shape=input_shape,\n",
    "     #                use_bias = True, kernel_initializer='random_uniform', bias_initializer='zeros'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer='random_uniform',bias_initializer=initialize_bias))\n",
    "\n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "\n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer='zeros')(L1_distance)\n",
    "\n",
    "     # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "\n",
    "    # return the model\n",
    "    return siamese_net\n",
    "    \n",
    "model = get_siamese_model((2, 30, 1))\n",
    "model.summary()\n",
    "optimizer = Adam(lr = 0.00006)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLossLayer(Layer):\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def triplet_loss(self, inputs):\n",
    "        \"\"\"\n",
    "        Implementation of the triplet loss as defined by formula (3)\n",
    "\n",
    "        Arguments:\n",
    "        y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "        y_pred -- python list containing three objects:\n",
    "                anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "                positive -- the encodings for the positive images, of shape (None, 128)\n",
    "                negative -- the encodings for the negative images, of shape (None, 128)\n",
    "\n",
    "        Returns:\n",
    "        loss -- real number, value of the loss\n",
    "        \"\"\"\n",
    "        anchor, positive, negative = inputs\n",
    "        ### START CODE HERE ### (≈ 4 lines)\n",
    "        # Step 1: Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1\n",
    "        pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=-1)\n",
    "        print(pos_dist)\n",
    "        # Step 2: Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "      #  neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=-1)\n",
    "\n",
    "        hardest_positive_dist = tf.reduce_max(tf.square(tf.subtract(anchor, positive)), axis=-1)\n",
    "        print(hardest_positive_dist)\n",
    "        tf.summary.scalar(\"hardest_positive_dist\", tf.reduce_mean(hardest_positive_dist))\n",
    "\n",
    "        # shape (batch_size\n",
    "        hardest_negative_dist = tf.reduce_min(tf.subtract(anchor, negative), axis=-1)\n",
    "        tf.summary.scalar(\"hardest_negative_dist\", tf.reduce_mean(hardest_negative_dist))\n",
    "\n",
    "         # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
    "        triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + self.alpha, 0.0)\n",
    "\n",
    "        # Get final mean triplet loss\n",
    "        triplet_loss = tf.reduce_mean(triplet_loss)\n",
    "\n",
    "        # Step 3: subtract the two previous distances and add alpha.\n",
    "       # basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
    "        # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "        loss = tf.reduce_sum(tf.maximum(triplet_loss, 0))\n",
    "        ### END CODE HERE ###\n",
    "        return loss\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"triplet_loss_layer_9/Sum:0\", shape=(None,), dtype=float32)\n",
      "Tensor(\"triplet_loss_layer_9/Max:0\", shape=(None,), dtype=float32)\n",
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 200, 30, 2, 2 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 30, 2, 2)     0           input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 30, 2, 32)    96          lambda_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 30, 2, 64)    18496       conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 30, 2, 128)   8320        conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 200, 7680)    0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_191 (Conv1D)             (None, 200, 128)     983168      lambda_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_192 (Conv1D)             (None, 200, 128)     32896       conv1d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 200, 128)     0           conv1d_192[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_121 (SpatialD (None, 200, 128)     0           activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_193 (Conv1D)             (None, 200, 128)     32896       spatial_dropout1d_121[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 200, 128)     0           conv1d_193[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_122 (SpatialD (None, 200, 128)     0           activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_194 (Conv1D)             (None, 200, 128)     16512       conv1d_191[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_71 (Add)                    (None, 200, 128)     0           conv1d_194[0][0]                 \n",
      "                                                                 spatial_dropout1d_122[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 200, 128)     0           add_71[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_195 (Conv1D)             (None, 200, 128)     32896       activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 200, 128)     0           conv1d_195[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_123 (SpatialD (None, 200, 128)     0           activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_196 (Conv1D)             (None, 200, 128)     32896       spatial_dropout1d_123[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 200, 128)     0           conv1d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_124 (SpatialD (None, 200, 128)     0           activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_197 (Conv1D)             (None, 200, 128)     16512       activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_72 (Add)                    (None, 200, 128)     0           conv1d_197[0][0]                 \n",
      "                                                                 spatial_dropout1d_124[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 200, 128)     0           add_72[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_198 (Conv1D)             (None, 200, 128)     32896       activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 200, 128)     0           conv1d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_125 (SpatialD (None, 200, 128)     0           activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_199 (Conv1D)             (None, 200, 128)     32896       spatial_dropout1d_125[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 200, 128)     0           conv1d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_126 (SpatialD (None, 200, 128)     0           activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_200 (Conv1D)             (None, 200, 128)     16512       activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_73 (Add)                    (None, 200, 128)     0           conv1d_200[0][0]                 \n",
      "                                                                 spatial_dropout1d_126[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, 200, 128)     0           add_73[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_201 (Conv1D)             (None, 200, 128)     32896       activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, 200, 128)     0           conv1d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_127 (SpatialD (None, 200, 128)     0           activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_202 (Conv1D)             (None, 200, 128)     32896       spatial_dropout1d_127[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, 200, 128)     0           conv1d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_128 (SpatialD (None, 200, 128)     0           activation_191[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_203 (Conv1D)             (None, 200, 128)     16512       activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_74 (Add)                    (None, 200, 128)     0           conv1d_203[0][0]                 \n",
      "                                                                 spatial_dropout1d_128[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, 200, 128)     0           add_74[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_204 (Conv1D)             (None, 200, 128)     32896       activation_192[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, 200, 128)     0           conv1d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_129 (SpatialD (None, 200, 128)     0           activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_205 (Conv1D)             (None, 200, 128)     32896       spatial_dropout1d_129[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_194 (Activation)     (None, 200, 128)     0           conv1d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_130 (SpatialD (None, 200, 128)     0           activation_194[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_206 (Conv1D)             (None, 200, 128)     16512       activation_192[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_75 (Add)                    (None, 200, 128)     0           conv1d_206[0][0]                 \n",
      "                                                                 spatial_dropout1d_130[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_195 (Activation)     (None, 200, 128)     0           add_75[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_207 (Conv1D)             (None, 200, 128)     32896       activation_195[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_196 (Activation)     (None, 200, 128)     0           conv1d_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_131 (SpatialD (None, 200, 128)     0           activation_196[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_208 (Conv1D)             (None, 200, 128)     32896       spatial_dropout1d_131[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, 200, 128)     0           conv1d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_132 (SpatialD (None, 200, 128)     0           activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_77 (Add)                    (None, 200, 128)     0           spatial_dropout1d_122[0][0]      \n",
      "                                                                 spatial_dropout1d_124[0][0]      \n",
      "                                                                 spatial_dropout1d_126[0][0]      \n",
      "                                                                 spatial_dropout1d_128[0][0]      \n",
      "                                                                 spatial_dropout1d_130[0][0]      \n",
      "                                                                 spatial_dropout1d_132[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 128)          0           add_77[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 256)          33024       lambda_33[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,520,416\n",
      "Trainable params: 1,520,416\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_21\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_input (InputLayer)       (None, 200, 30, 2, 2 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_input (InputLayer)     (None, 200, 30, 2, 2 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_input (InputLayer)     (None, 200, 30, 2, 2 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_20 (Model)                (None, 256)          1520416     anchor_input[0][0]               \n",
      "                                                                 positive_input[0][0]             \n",
      "                                                                 negative_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "triplet_loss_layer (TripletLoss [(None, 256), (None, 0           model_20[1][0]                   \n",
      "                                                                 model_20[2][0]                   \n",
      "                                                                 model_20[3][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,520,416\n",
      "Trainable params: 1,520,416\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaushalyakularatnam/Software/anaconda3/anaconda3/envs/py37/lib/python3.7/site-packages/keras/engine/training_utils.py:819: UserWarning: Output triplet_loss_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to triplet_loss_layer.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "def embedding_model(num_frames, h, w, c, input_shape, dimensions, include_top=False, pooling=None, classes=1):  \n",
    "    inp = Input(shape=(num_frames, h, w, c))\n",
    "    out = Lambda(lambda y: K.reshape(y, (-1, h, w, c)))(inp)\n",
    "    out = Conv2D(32, kernel_size=(1,1), activation='relu', strides=1, padding=\"same\", input_shape=input_shape,\n",
    "                   use_bias = True, kernel_initializer=initialize_weights, bias_initializer=initialize_bias)(out)\n",
    "   # out = MaxPooling2D()(out)\n",
    "    out = Conv2D(64, kernel_size=(3,3), activation='relu', strides=1, padding=\"same\",\n",
    "                   use_bias = True, kernel_initializer=initialize_weights, bias_initializer=initialize_bias)(out)\n",
    "    out = Conv2D(128, kernel_size=(1,1), activation='relu', strides=1, padding=\"same\",\n",
    "                   use_bias = True, kernel_initializer=initialize_weights, bias_initializer=initialize_bias)(out)\n",
    "    num_features_cnn = np.prod(K.int_shape(out)[1:])\n",
    "    out = Lambda(lambda y: K.reshape(y, (-1, num_frames, num_features_cnn)))(out)\n",
    "    out = TCN(nb_filters=128)(out)\n",
    "   # out = Flatten()(out)\n",
    "    out = Dense(256, activation=None, kernel_regularizer=l2(1e-3), kernel_initializer='he_uniform')(out)\n",
    "   # L1_layer = Lambda(lambda tensors:tf.math.l2_normalize(tensors, axis=1))\n",
    "    #out = L1_layer(out)    \n",
    "    return Model(inputs=inp, outputs=out)\n",
    "\n",
    "\n",
    "def triplets_model(input_shape, embedding, include_top=False, pooling=None):\n",
    "\n",
    "    anchor_input = Input(shape=input_shape, name='anchor_input')\n",
    "    positive_input = Input(shape=input_shape, name='positive_input')\n",
    "    negative_input = Input(shape=input_shape, name='negative_input')\n",
    "\n",
    "    # Get the embedded values\n",
    "    encoded_a = embedding(anchor_input)\n",
    "    encoded_p = embedding(positive_input)\n",
    "    encoded_n = embedding(negative_input)\n",
    "    \n",
    "    #TripletLoss Layer\n",
    "    loss_layer = TripletLossLayer(alpha=0.2,name='triplet_loss_layer')([encoded_a,encoded_p,encoded_n])\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    triplet_net = Model(inputs=[anchor_input,positive_input,negative_input],outputs=loss_layer)\n",
    "    \n",
    "    return triplet_net\n",
    "\n",
    "build_embedding = embedding_model(200, 30, 2, 2, input_shape=(200, 30, 2, 2), dimensions=60)\n",
    "build_triplet = triplets_model(input_shape=(200, 30, 2, 2), embedding=build_embedding)\n",
    "optimizer = Adam(lr = 0.00006)\n",
    "build_triplet.compile(loss=None,optimizer=optimizer)\n",
    "build_embedding.summary()\n",
    "build_triplet.summary()\n",
    "#plot_model(build_triplet, show_shapes=True, show_layer_names=True, to_file='02 model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Triplet Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triplet_batch(batch_size, samples, labels, anchor):\n",
    "    n_examples, time_axis, d, w, h = samples.shape\n",
    "    triplets = [np.zeros((batch_size, 5, 2, 13)) for i in range(3)]\n",
    "    print(triplets[0])\n",
    "    \n",
    "    # initialize vector for the targets\n",
    "    targets = np.ones((batch_size,3))\n",
    "    targets[:,2] = 0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        idx_p = rng.randint(0, n_examples_p)\n",
    "        idx_n = rng.randint(0, n_examples_n)\n",
    "        triplets[0][i,:,:,:] = anchor.reshape(w, h, 2)\n",
    "        triplets[1][i,:,:,:] = positive_samples[idx_p].reshape(w, h, 2)\n",
    "        triplets[2][i,:,:,:] = negative_samples[idx_n].reshape(w, h, 2)\n",
    "    return [triplets[0], triplets[1], triplets[2]], [targets[:,0], targets[:,1], targets[:,2]]\n",
    "\n",
    "def get_triplet_batch_spoof(batch_size, lob_states, labels):\n",
    "    n_examples, t, h, w, d = lob_states.shape\n",
    "    triplets = [np.zeros((batch_size, t, h, w, d)) for i in range(3)]\n",
    "   \n",
    "    labels_b = labels[labels==1]\n",
    "    labels_s = labels[labels==2]\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        idx_a = labels_b.sample(1)\n",
    "        idx_p = labels_b.sample(1)\n",
    "        idx_n = labels_s.sample(1)\n",
    "        triplets[0][i,:,:,:] = lob_states[idx_a]\n",
    "        triplets[1][i,:,:,:] = lob_states[idx_p]\n",
    "        triplets[2][i,:,:,:] = lob_states[idx_n]\n",
    "    return [triplets[0], triplets[1], triplets[2]]\n",
    "\n",
    "def get_triplet_batch_new(batch_size, lob_states):\n",
    "    n_samples, states, h, w, d = lob_states.shape\n",
    "    triplets = [np.zeros((batch_size, states, h, w, d)) for i in range(3)]\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        idx_p = rng.randint(0, n_samples)\n",
    "        idx_n = rng.randint(0, n_samples)\n",
    "        idx_a = rng.randint(0, n_samples)\n",
    "        triplets[0][i,:,:,:,:] = lob_states[idx_a]\n",
    "        triplets[1][i,:,:,:,:] = lob_states[idx_p]\n",
    "        triplets[2][i,:,:,:,:] = lob_states[idx_n]\n",
    "    return [triplets[0], triplets[1], triplets[2]]\n",
    "\n",
    "def triplet_generator(batch_size, positive_samples, neg_train, anchor):\n",
    "    while True:\n",
    "        inputs, targets = get_triplet_batch(batch_size, positive_samples, neg_train, anchor)\n",
    "        label = None\n",
    "        yield ({'anchor_input': inputs[0], 'positive_input': inputs[1], 'negative_input': inputs[2]},\\\n",
    "               [targets[0], targets[1], targets[2]])\n",
    "        \n",
    "def data_generator(batch_size, positive_samples, negative_samples, anchor, class_val):\n",
    "    # Currently simply using the triplet generator function but can change this to something else \n",
    "    while True:\n",
    "        inputs, targets = get_triplet_batch(batch_size, positive_samples, negative_samples, anchor)\n",
    "        return inputs[class_val], targets[class_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process!\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "t_start = time.time()\n",
    "#dummy_target = [np.zeros((batch_size,15)) for i in range(3)]\n",
    "for i in range(1, n_iter+1):\n",
    "    triplets = get_triplet_batch_spoof(1, X_train, Y_train)\n",
    "    loss = build_triplet.train_on_batch(triplets, None)\n",
    "    n_iteration += 1\n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"\\n ------------- \\n\")\n",
    "        print(\"[{3}] Time for {0} iterations: {1:.1f} mins, Train Loss: {2}\"\\\n",
    "              .format(i, (time.time()-t_start)/60.0,loss,n_iteration))\n",
    "        probs,yprob = compute_probs(build_embedding ,X_test[:n_val,:,:,:,:],Y_test[:n_val])\n",
    "        #fpr, tpr, thresholds,auc = compute_metrics(probs,yprob)\n",
    "        #draw_roc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation / Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist(a,b):\n",
    "    return np.sum(np.square(a-b))\n",
    "\n",
    "def compute_probs(network,X,Y):\n",
    "    '''\n",
    "    Input\n",
    "        network : current NN to compute embeddings\n",
    "        X : tensor of shape (m,w,h,1) containing pics to evaluate\n",
    "        Y : tensor of shape (m,) containing true class\n",
    "        \n",
    "    Returns\n",
    "        probs : array of shape (m,m) containing distances\n",
    "    \n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "    nbevaluation = int(m*(m-1)/2)\n",
    "    probs = np.zeros((nbevaluation))\n",
    "    y = np.zeros((nbevaluation))\n",
    "    \n",
    "    #Compute all embeddings for all pics with current network\n",
    "    embeddings = network.predict(X)\n",
    "    \n",
    "    size_embedding = embeddings.shape[1]\n",
    "    \n",
    "    #For each pics of our dataset\n",
    "    k = 0\n",
    "    for i in range(m):\n",
    "            #Against all other images\n",
    "            for j in range(i+1,m):\n",
    "                #compute the probability of being the right decision : it should be 1 for right class, 0 for all other classes\n",
    "                probs[k] = -compute_dist(embeddings[i,:],embeddings[j,:])\n",
    "                if (Y[i]==Y[j]):\n",
    "                    y[k] = 1\n",
    "                    #print(\"{3}:{0} vs {1} : {2}\\tSAME\".format(i,j,probs[k],k))\n",
    "                else:\n",
    "                    y[k] = 0\n",
    "                    #print(\"{3}:{0} vs {1} : \\t\\t\\t{2}\\tDIFF\".format(i,j,probs[k],k))\n",
    "                k += 1\n",
    "    return probs,y\n",
    "#probs,yprobs = compute_probs(network,x_test_origin[:10,:,:,:],y_test_origin[:10])\n",
    "\n",
    "def compute_metrics(probs,yprobs):\n",
    "    '''\n",
    "    Returns\n",
    "        fpr : Increasing false positive rates such that element i is the false positive rate of predictions with score >= thresholds[i]\n",
    "        tpr : Increasing true positive rates such that element i is the true positive rate of predictions with score >= thresholds[i].\n",
    "        thresholds : Decreasing thresholds on the decision function used to compute fpr and tpr. thresholds[0] represents no instances being predicted and is arbitrarily set to max(y_score) + 1\n",
    "        auc : Area Under the ROC Curve metric\n",
    "    '''\n",
    "    # calculate AUC\n",
    "    auc = roc_auc_score(yprobs, probs)\n",
    "    # calculate roc curve\n",
    "    fpr, tpr, thresholds = roc_curve(yprobs, probs)\n",
    "    \n",
    "    return fpr, tpr, thresholds,auc\n",
    "\n",
    "def compute_interdist(network):\n",
    "    '''\n",
    "    Computes sum of distances between all classes embeddings on our reference test image: \n",
    "        d(0,1) + d(0,2) + ... + d(0,9) + d(1,2) + d(1,3) + ... d(8,9)\n",
    "        A good model should have a large distance between all theses embeddings\n",
    "        \n",
    "    Returns:\n",
    "        array of shape (nb_classes,nb_classes) \n",
    "    '''\n",
    "    res = np.zeros((nb_classes,nb_classes))\n",
    "    \n",
    "    ref_images = np.zeros((nb_classes,cols,rows,1))\n",
    "    \n",
    "    #generates embeddings for reference images\n",
    "    for i in range(nb_classes):\n",
    "        ref_images[i,:,:,:] = list_of_test[i][0,:,:,:1]\n",
    "    ref_embeddings = network.predict(ref_images)\n",
    "    \n",
    "    for i in range(nb_classes):\n",
    "        for j in range(nb_classes):\n",
    "            res[i,j] = dist(ref_embeddings[i],ref_embeddings[j])\n",
    "    return res\n",
    "\n",
    "def draw_interdist(network, n_iteration):\n",
    "    interdist = compute_interdist(network)\n",
    "    \n",
    "    data = []\n",
    "    for i in range(nb_classes):\n",
    "        data.append(np.delete(interdist[i,:],[i]))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('Evaluating embeddings distance from each other after {0} iterations'.format(n_iteration))\n",
    "    ax.set_ylim([0,3])\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Distance')\n",
    "    ax.boxplot(data,showfliers=False,showbox=True)\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.xticks(locs,np.arange(nb_classes))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def find_nearest(array,value):\n",
    "    idx = np.searchsorted(array, value, side=\"left\")\n",
    "    if idx > 0 and (idx == len(array) or math.fabs(value - array[idx-1]) < math.fabs(value - array[idx])):\n",
    "        return array[idx-1],idx-1\n",
    "    else:\n",
    "        return array[idx],idx\n",
    "    \n",
    "def draw_roc(fpr, tpr,thresholds):\n",
    "    #find threshold\n",
    "    targetfpr=1e-3\n",
    "    _, idx = find_nearest(fpr,targetfpr)\n",
    "    threshold = thresholds[idx]\n",
    "    recall = tpr[idx]\n",
    "    \n",
    "    \n",
    "    # plot no skill\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    # plot the roc curve for the model\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.title('AUC: {0:.3f}\\nSensitivity : {2:.1%} @FPR={1:.0e}\\nThreshold={3})'.format(auc,targetfpr,recall,abs(threshold) ))\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing on an untrained network\n",
    "print(x_test[:1500,:,:,:1].shape)\n",
    "\n",
    "probs, yprob = compute_probs(build_embedding ,x_test[:1500,:,:,:1], y_test[:1500])\n",
    "fpr, tpr, thresholds,auc = compute_metrics(probs, yprob)\n",
    "draw_roc(fpr, tpr,thresholds)\n",
    "draw_interdist(build_embedding,n_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawTestImage(network, images, refidx=500):\n",
    "    '''\n",
    "    Evaluate some pictures vs some samples in the test set\n",
    "        image must be of shape(1,w,h,c)\n",
    "    \n",
    "    Returns\n",
    "        scores : resultat des scores de similarités avec les images de base => (N)\n",
    "    \n",
    "    '''\n",
    "    N=4\n",
    "    _, w,h,c = list_of_test[0].shape\n",
    "    nbimages=images.shape[0]\n",
    "    \n",
    "    #generates embedings for given images\n",
    "    image_embedings = network.predict(images)\n",
    "    \n",
    "    #generates embedings for reference images\n",
    "    ref_images = np.zeros((nb_classes,w,h,c))\n",
    "    for i in range(nb_classes):\n",
    "        ref_images[i,:,:,:1] = list_of_test[i][refidx,:,:,:1]\n",
    "    ref_embedings = network.predict(ref_images)\n",
    "            \n",
    "    for i in range(nbimages):\n",
    "        #Prepare the figure\n",
    "        fig=plt.figure(figsize=(16,2))\n",
    "        subplot = fig.add_subplot(1,nb_classes+1,1)\n",
    "        axis(\"off\")\n",
    "        plotidx = 2\n",
    "            \n",
    "        #Draw this image    \n",
    "        #plt.imshow(images[i,:,:,0],vmin=0, vmax=1,cmap='Greys')\n",
    "        plt.matshow(images[i,:,:,0], cmap='gray')\n",
    "        #sns.heatmap(images[i,:,:,0], vmin=0, vmax=5000, cmap='Greens', center=None,\n",
    "           #             robust=False, annot=None, fmt='.2g', annot_kws=None, linewidths=0, linecolor='white', \n",
    "           #             cbar=True, cbar_kws=None, cbar_ax=None, square=False, xticklabels=True, \n",
    "           #             yticklabels=True, mask=None, ax=None)\n",
    "        subplot.title.set_text(\"Test image\")\n",
    "            \n",
    "        for ref in range(nb_classes):\n",
    "            #Compute distance between this images and references\n",
    "            dist = compute_dist(image_embedings[i,:],ref_embedings[ref,:])\n",
    "            #Draw\n",
    "            subplot = fig.add_subplot(1,nb_classes+1,plotidx)\n",
    "            axis(\"off\")\n",
    "            #plt.imshow(ref_images[ref,:,:,0],vmin=0, vmax=1,cmap='Greys')\n",
    "            plt.matshow(ref_images[ref,:,:,0], vmin=0, vmax=200000, cmap='gray')\n",
    "            #sns.heatmap(ref_images[ref,:,:,0].reshape(2,30).T, vmin=0, vmax=5000, cmap='Greens', center=None,\n",
    "             #           robust=False, annot=None, fmt='.2g', annot_kws=None, linewidths=0, linecolor='white', \n",
    "              #          cbar=True, cbar_kws=None, cbar_ax=None, square=False, xticklabels=True, \n",
    "               #         yticklabels=True, mask=None, ax=None)\n",
    "            subplot.title.set_text((\"Class {0}\\n{1:.3e}\".format(ref,dist)))\n",
    "            plotidx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    DrawTestImage(build_embedding,np.expand_dims(list_of_train[i][230,:,:,:1],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full evaluation\n",
    "probs,yprob = compute_probs(build_embedding,x_test,y_test)\n",
    "fpr, tpr, thresholds,auc = compute_metrics(probs,yprob)\n",
    "draw_roc(fpr, tpr,thresholds)\n",
    "draw_interdist(build_embedding,n_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    DrawTestImage(build_embedding,np.expand_dims(list_of_train[i][680,:,:,:],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(model_cp_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=2)\n",
    "callbacks_list = [checkpoint, early]  # early\n",
    "\n",
    "gen_tr = triplet_generator(batch_size, pos_train, neg_train, spoof_ground_truth.spoof_step1_truth1)\n",
    "\n",
    "history = triplet_model.fit_generator(gen_tr, \n",
    "                              epochs=10, \n",
    "                              verbose=1, \n",
    "                              workers=4,\n",
    "                              steps_per_epoch=200, \n",
    "                              validation_steps=20, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds  = []\n",
    "train_file_names = []\n",
    "for i in range(1, n_iter+1):\n",
    "    inputs, targets = data_generator(32, pos_train, neg_train, spoof_ground_truth.spoof_step1_truth1, 1)\n",
    "    predicts = triplet_model.predict(inputs)\n",
    "    print(predicts[0])\n",
    "    predicts = predicts.tolist()\n",
    "    train_preds += predicts\n",
    "    targets = targets.tolist()\n",
    "    train_file_names += targets\n",
    "train_preds = np.array(train_preds)\n",
    "\n",
    "\n",
    "test_preds  = []\n",
    "test_file_names = []\n",
    "for i in range(1, n_iter+1):\n",
    "    inputs, targets = data_generator(32, pos_val, neg_val, spoof_ground_truth.spoof_step1_truth1, 1)\n",
    "    if i % evaluate_every == 0:\n",
    "        predicts = embedding_model.predict(inputs)\n",
    "        predicts = predicts.tolist()\n",
    "        test_preds += predicts\n",
    "        targets = targets.tolist()\n",
    "        test_file_names += targets\n",
    "test_preds = np.array(test_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=6)\n",
    "neigh.fit(train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_test, neighbors_test = neigh.kneighbors(test_preds)\n",
    "distances_test, neighbors_test = distances_test.tolist(), neighbors_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_te = triplet_generator_new(batch_size, pos_val, neg_val, spoof_ground_truth.spoof_step1_truth1)\n",
    "predictions = triplet_model.embedding_model.predict(gen_te, steps=1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.legend()\n",
    "# plt.show()\n",
    "def eva_plot(History, epoch):\n",
    "    plt.figure(figsize=(20,10))\n",
    "#     sns.lineplot(range(1, epoch+1), History.history['acc'], label='Train Accuracy')\n",
    "#     sns.lineplot(range(1, epoch+1), History.history['val_acc'], label='Test Accuracy')\n",
    "#     plt.legend(['train', 'validaiton'], loc='upper left')\n",
    "#     plt.ylabel('accuracy')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.show()\n",
    "    plt.figure(figsize=(20,10))\n",
    "    sns.lineplot(range(1, epoch+1), History.history['loss'], label='Train loss')\n",
    "    sns.lineplot(range(1, epoch+1), History.history['val_loss'], label='Test loss')\n",
    "    plt.legend(['train', 'validaiton'], loc='upper left')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title(\"Loss Graph\")\n",
    "    plt.show()\n",
    "    \n",
    "eva_plot(history, 4)\n",
    "\n",
    "def val_plot(History, epoch):\n",
    "    plt.figure(figsize=(20,10))\n",
    "#     sns.lineplot(range(1, epoch+1), History.history['acc'], label='Train Accuracy')\n",
    "#     sns.lineplot(range(1, epoch+1), History.history['val_acc'], label='Test Accuracy')\n",
    "#     plt.legend(['train', 'validaiton'], loc='upper left')\n",
    "#     plt.ylabel('accuracy')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.show()\n",
    "    plt.figure(figsize=(20,10))\n",
    "    sns.lineplot(range(1, epoch+1), History.history['model_8_loss'], label='Embedding Train loss')\n",
    "    sns.lineplot(range(1, epoch+1), History.history['val_model_8_loss'], label='Embedding Test loss')\n",
    "    plt.legend(['train', 'validaiton'], loc='upper left')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title(\"Loss Graph\")\n",
    "    plt.show()\n",
    "    \n",
    "val_plot(history, 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss1(y_true, y_pred, alpha=0.4):\n",
    "    print(\"for distance branch, y_pred.shape:  \", y_pred)       # [Batch_dim, vec_dim*3]\n",
    "\n",
    "    vec_len = y_pred.shape.as_list()[-1]\n",
    "\n",
    "    anchor = y_pred[:, :int(vec_len/3)]\n",
    "    positve = y_pred[:, int(vec_len/3):int(vec_len*2/3)]\n",
    "    negative = y_pred[:, int(vec_len*2/3):]\n",
    "\n",
    "    pos_dist = K.sum(K.square(anchor - positve), axis=1)\n",
    "    neg_dist = K.sum(K.square(anchor - negative), axis=1)\n",
    "\n",
    "    loss = K.maximum(0., pos_dist - neg_dist + alpha)\n",
    "\n",
    "    return loss\n",
    "a = tf.compat.v1.random_normal([5, 60], mean=6, stddev=0.1, seed = 1)\n",
    "\n",
    "with tf.compat.v1.Session() as test:\n",
    "    tf.compat.v1.set_random_seed(1)\n",
    "    y_true = (None, None, None)\n",
    "    a = tf.compat.v1.random_normal([5, 60], mean=6, stddev=0.1, seed = 1)\n",
    "    print(a)\n",
    "    p = tf.compat.v1.random_normal([5, 60], mean=1, stddev=1, seed = 1)\n",
    "    n = tf.compat.v1.random_normal([5, 60], mean=3, stddev=4, seed = 1)\n",
    "    merged = concatenate([a, p, n], axis=-1)\n",
    "    y_pred = (tf.compat.v1.random_normal([3, 60], mean=6, stddev=0.1, seed = 1),\n",
    "              tf.compat.v1.random_normal([3, 60], mean=1, stddev=1, seed = 1),\n",
    "              tf.compat.v1.random_normal([3, 60], mean=3, stddev=4, seed = 1), \n",
    "              tf.compat.v1.random_normal([3, 60], mean=3, stddev=4, seed = 1))\n",
    "    loss = triplet_loss1(y_true, merged)\n",
    "    \n",
    "    print(\"loss = \" + str(loss.eval()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_every = 20 # interval for evaluating on one-shot tasks\n",
    "loss_every = 20 # interval for printing loss (iterations)\n",
    "batch_size = 32\n",
    "n_iter = 2\n",
    "N_way = 20 # how many classes for testing one-shot tasks>\n",
    "n_val = 250 # how many one-shot tasks to validate on?\n",
    "best = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "t_start = time.time()\n",
    "for i in range(1, n_iter+1):\n",
    "    (inputs, targets) = get_triplet_batch(batch_size, pos_train, neg_train, spoof_ground_truth.spoof_step1_truth1)\n",
    "    #(inp, tar) = triplet_generator()\n",
    "    loss = triplet_model.model.train_on_batch(inputs, targets)\n",
    "    print(loss)\n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"\\n ------------- \\n\")\n",
    "        print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()-t_start)/60.0))\n",
    "        print(\"Train Loss: {0}\".format(loss)) \n",
    "        val_acc = test_oneshot(model, N_way, n_val, verbose=True)\n",
    "\n",
    "  \n",
    "        triplet_model.model.save_weights(os.path.join(model_path, 'weights_triplet.{}.h5'.format(i)))\n",
    "        if val_acc >= best:\n",
    "            print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "            best = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "data = [[1,2,3],[4,5,6]]\n",
    "data_np = np.asarray(data, np.float32)\n",
    "\n",
    "data_tf = tf.convert_to_tensor(data_np, np.float32)\n",
    "\n",
    "data_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = triplet_generator(batch_size, pos_train, neg_train, spoof_ground_truth.spoof_step1_truth1)\n",
    "val_generator = triplet_generator(batch_size, pos_val, neg_val, spoof_ground_truth.spoof_step1_truth1)\n",
    "\n",
    "model.fit(generator=train_generator, steps_per_epoch=20, epochs=100,\n",
    "                        validation_data=val_generator,\n",
    "                        validation_steps=10,\n",
    "                        verbose=1) #callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "t_start = time.time()\n",
    "for i in range(1, n_iter+1):\n",
    "    (inputs, targets) = get_triplet_batch(batch_size, pos_train, neg_train, spoof_ground_truth.spoof_step1_truth1)\n",
    "    #(inp, tar) = triplet_generator()\n",
    "    loss = triplet_model.model.train_on_batch(inputs, targets)\n",
    "    print(loss)\n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"\\n ------------- \\n\")\n",
    "        print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()-t_start)/60.0))\n",
    "        print(\"Train Loss: {0}\".format(loss)) \n",
    "      #  val_acc = test_oneshot(model, N_way, n_val, verbose=True)\n",
    "        triplet_model.model.save_weights(os.path.join(model_path, 'weights_triplet.{}.h5'.format(i)))\n",
    "       # if val_acc >= best:\n",
    "        #    print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "         #   best = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training_Pipeline(object):\n",
    "    def __init__(self):\n",
    "        self._birthdate = time.time()\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_batch(batch_size, positive_samples, negative_samples, anchor):\n",
    "        \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "        n_examples_p, d, w, h = positive_samples.shape\n",
    "        n_examples_n = negative_samples.shape[0]\n",
    "\n",
    "        # initialize 2 empty arrays for the input image batch\n",
    "        pairs = [np.zeros((batch_size, w, h, 1)) for i in range(2)]\n",
    "\n",
    "        # initialize vector for the targets\n",
    "        targets=np.zeros((batch_size,))\n",
    "\n",
    "        # make one half of it '1's, so 2nd half of batch has same class\n",
    "        targets[batch_size//2:] = 1\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            idx_p = rng.randint(0, n_examples_p)\n",
    "            idx_n = rng.randint(0, n_examples_n)\n",
    "            pairs[0][i,:,:,:] = anchor.reshape(w, h, 1)\n",
    "\n",
    "            if i >= batch_size // 2:\n",
    "                pairs[1][i,:,:,:] = positive_samples[idx_p].reshape(w, h, 1)\n",
    "            else:\n",
    "                pairs[1][i,:,:,:] = negative_samples[idx_n].reshape(w, h, 1)\n",
    "        return pairs, targets\n",
    "    \n",
    "    def make_oneshot_task(self, N, positive_samples, negative_samples, anchor):\n",
    "        \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
    "\n",
    "        n_examples_p, d, w, h = positive_samples.shape\n",
    "        n_examples_n = negative_samples.shape[0]\n",
    "\n",
    "        test_lob = np.asarray([anchor]*N).reshape(N, w, h, 1)\n",
    "\n",
    "       # print(test_lob)\n",
    "\n",
    "        p_index = rng.randint(0, n_examples_p, size=(1,))\n",
    "        n_index = rng.randint(0, n_examples_n, size=(N,))\n",
    "\n",
    "        support_set = negative_samples[n_index]\n",
    "       # print('n_index' + str(n_index))\n",
    "      #  print(support_set[0])\n",
    "      #  print(support_set[1])\n",
    "        support_set[0] = positive_samples[p_index]\n",
    "       # print('zero now')\n",
    "       # print(support_set[0])\n",
    "       # print('one now')\n",
    "      #  print(support_set[1])\n",
    "        support_set = support_set.reshape(N, w, h, 1)\n",
    "\n",
    "        targets = np.zeros((N,))\n",
    "        targets[0] = 1     \n",
    "       # print(targets)\n",
    "        targets, test_lob, support_set = shuffle(targets, test_lob, support_set)\n",
    "        pairs = [test_lob, support_set]\n",
    "        return pairs, targets\n",
    "\n",
    "    def test_oneshot(self, model, N, k, s = \"val\", verbose = 0):\n",
    "        \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "        n_correct = 0\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        if verbose:\n",
    "            print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
    "        for i in range(k):\n",
    "            if s == 'train':\n",
    "                inputs, targets = self.make_oneshot_task(N, pos_train, neg_train, spoof_ground_truth.spoof_step1_truth1)\n",
    "            else:\n",
    "                inputs, targets = self.make_oneshot_task(N, pos_val, neg_val, spoof_ground_truth.spoof_step1_truth1)\n",
    "            probs = model.predict(inputs)\n",
    "            #print(probs)\n",
    "            if np.argmax(probs) == np.argmax(targets):\n",
    "                n_correct+=1\n",
    "        percent_correct = (100.0 * n_correct / k)\n",
    "        if verbose:\n",
    "            print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
    "        return percent_correct\n",
    "    \n",
    "    \n",
    "    def generate(batch_size, s=\"train\"):\n",
    "        \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
    "        while True:\n",
    "            pairs, targets = get_batch(batch_size,s)\n",
    "            yield (pairs, targets)\n",
    "\n",
    "training_pipeline = Training_Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of concat image visualization\n",
    "np.set_printoptions(suppress=True)\n",
    "pairs, targets = training_pipeline.make_oneshot_task(15, pos_train, neg_train, spoof_ground_truth.spoof_step1_truth1)\n",
    "plot_oneshot_task(pairs, targets, 15)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting LOBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_oneshot_task(pairs, targets, N): \n",
    "    '''\n",
    "    Only prints out one before last element of a \n",
    "    '''\n",
    "    count = 0\n",
    "    fig, ax = plt.subplots(2, int(N/2), sharex='col', sharey='row', figsize=(15,15))\n",
    "    lob_matrix = []\n",
    "    lob_matrix.append(pairs[0][0])\n",
    "    for j in range(0,int(N)):\n",
    "        lob_matrix.append(pairs[1][j])\n",
    "     \n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "    \n",
    "    for i in range(0,2):\n",
    "        for j in range(0,int(N/2)):\n",
    "            sns.heatmap(lob_matrix[count][:,:,0].reshape(2,30).T, vmin=0, vmax=50000, cmap='Greens', center=None,\n",
    "                        robust=False, annot=None, fmt='.2g', annot_kws=None, linewidths=0, linecolor='white', \n",
    "                        cbar=True, cbar_kws=None, cbar_ax=cbar_ax, square=False, xticklabels=True, \n",
    "                        yticklabels=True, mask=None, ax=ax[i, j])\n",
    "            count = count + 1\n",
    "            print(lob_matrix[count][:,:,0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_oneshot_task(pairs, N):\n",
    "    if N/2 == 1:\n",
    "        end = int(N/2)\n",
    "    else:\n",
    "        end = int(N/2)\n",
    "    fig, ax = plt.subplots(2, end, sharex='col', sharey='row', figsize=(15,15))\n",
    "   # im = ax[0, 0].matshow(pairs[0][0].reshape(2,30).T)\n",
    "   # print(pairs[0][1])\n",
    "   # print(pairs[1][0])\n",
    "   # print(pairs[0][0].shape)\n",
    "    data_array = []\n",
    "    data_array.append(pairs[0][0])\n",
    "    #print(data_array[0][:,:,0])\n",
    "    for j in range(0,int(N)):\n",
    "        data_array.append(pairs[1][j])\n",
    "    #print(np.moveaxis(data_array, -1, 0).shape)\n",
    "   # print(data_array[0])\n",
    "    count = 0\n",
    "    initial_pair = 0\n",
    "    for i in range(0,2):\n",
    "        for j in range(0,int(N/2)):\n",
    "            sns.heatmap(data_array[count][:,:,0].reshape(2,30).T, vmin=None, vmax=None, cmap=None, center=None,\n",
    "                robust=False, annot=None, fmt='.2g', annot_kws=None, \n",
    "                linewidths=0, linecolor='white', cbar=True, cbar_kws=None, \n",
    "                cbar_ax=None, square=False, xticklabels='auto',\n",
    "                yticklabels='auto', mask=None, ax=ax[i, j], **kwargs)\n",
    "           # im = ax[i, j].matshow(data_array[count][:,:,0].reshape(2,30).T, cmap='BuPu')\n",
    "            print(i)\n",
    "            print(j)\n",
    "            ax[i, j].text(0.5, 0.5, str((i, j)), fontsize=18, ha='right')\n",
    "            count = count + 1\n",
    "            initial_pair = 1\n",
    "    \n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "    print(im)\n",
    "    fig.colorbar(im, cax=cbar_ax)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_oneshot_task(pairs, N):\n",
    "    if N/2 == 1:\n",
    "        end = int(N/2)\n",
    "    else:\n",
    "        end = int(N/2)\n",
    "    fig, ax = plt.subplots(2, end, sharex='col', sharey='row', figsize=(15,15))\n",
    "   # im = ax[0, 0].matshow(pairs[0][0].reshape(2,30).T)\n",
    "   # print(pairs[0][1])\n",
    "   # print(pairs[1][0])\n",
    "    data_array = pairs[0][0]\n",
    "    for j in range(0,int(N)):\n",
    "        data_array = np.append(data_array, pairs[1][j], axis=-1)\n",
    "    print(data_array)\n",
    "    print(data_array.shape)\n",
    "    print(np.moveaxis(data_array, -1, 0).shape)\n",
    "    count = 0\n",
    "    initial_pair = 0\n",
    "    for i in range(0,2):\n",
    "        for j in range(0,int(end)):\n",
    "            im = ax[i, j].matshow(pairs[initial_pair][count].reshape(2,30).T, cmap='BuPu')\n",
    "            ax[i, j].text(0.5, 0.5, str((i, j)), fontsize=18, ha='right')\n",
    "            count = count + 1\n",
    "            initial_pair = 1\n",
    "    \n",
    "    fig.subplots_adjust(right=0.8)\n",
    "    cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
    "    print(im)\n",
    "    fig.colorbar(im, cax=cbar_ax)\n",
    "    plt.show()\n",
    "#(j-start)*(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_oneshot_task(pairs, N):\n",
    "    fig,(ax1,ax2) = plt.subplots(nrows=1, ncols=N, figsize=(15,15))\n",
    "    ax1.matshow(pairs[0][0].reshape(30,2), cmap='gray')\n",
    "    img = concat_images(pairs[1], N)\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax2.matshow(img,cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "#evaluate_every = 2 # interval for evaluating on one-shot tasks\n",
    "#batch_size = 32\n",
    "#n_iter = 20 # No. of training iterations\n",
    "#N_way = 4 # how many classes for testing one-shot tasks\n",
    "#n_val = 10 # how many one-shot tasks to validate on\n",
    "#best = -1\n",
    "#np.set_printoptions(suppress=True)\n",
    "\n",
    "evaluate_every = 10 # interval for evaluating on one-shot tasks\n",
    "loss_every = 20 # interval for printing loss (iterations)\n",
    "batch_size = 32\n",
    "n_iter = 20000\n",
    "N_way = 20 # how many classes for testing one-shot tasks>\n",
    "n_val = 250 # how many one-shot tasks to validate on?\n",
    "best = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "t_start = time.time()\n",
    "for i in range(1, n_iter+1):\n",
    "    inputs, targets = training_pipeline.get_batch(batch_size, pos_train, neg_train,\\\n",
    "                                                     spoof_ground_truth.spoof_step1_truth1)\n",
    "    loss = model.train_on_batch(inputs, targets)\n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"\\n ------------- \\n\")\n",
    "        print(\"Time for {0} iterations: {1} mins\".format(i, (time.time()-t_start)/60.0))\n",
    "        print(\"Train Loss: {0}\".format(loss)) \n",
    "        val_acc = training_pipeline.test_oneshot(model, N_way, n_val, verbose=True)\n",
    "        model.save_weights(os.path.join(model_path, 'weights.{}.h5'.format(i)))\n",
    "        if val_acc >= best:\n",
    "            print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "            best = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(os.path.join(model_path, \"weights.20000.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline model based on nearest neighbors using euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbour_correct(pairs,targets):\n",
    "    \"\"\"returns 1 if nearest neighbour gets the correct answer for a one-shot task\n",
    "        given by (pairs, targets)\"\"\"\n",
    "    L2_distances = np.zeros_like(targets)\n",
    "    for i in range(len(targets)):\n",
    "        L2_distances[i] = np.sum(np.sqrt(pairs[0][i]**2 - pairs[1][i]**2))\n",
    "    if np.argmin(L2_distances) == np.argmax(targets):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nn_accuracy(N_ways,n_trials):\n",
    "    \"\"\"Returns accuracy of NN approach \"\"\"\n",
    "    print(\"Evaluating nearest neighbour on {} unique {} way one-shot learning tasks ...\".format(n_trials,N_ways))\n",
    "\n",
    "    n_right = 0\n",
    "    \n",
    "    for i in range(n_trials):\n",
    "        pairs,targets = training_pipeline.make_oneshot_task(N_ways,pos_val, neg_val,\\\n",
    "                                                            spoof_ground_truth.spoof_step1_truth1)\n",
    "        correct = nearest_neighbour_correct(pairs,targets)\n",
    "        n_right += correct\n",
    "    return 100.0 * n_right / n_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ways = np.arange(1,15,2)\n",
    "resume =  False\n",
    "trials = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_accs, train_accs,nn_accs = [], [], []\n",
    "for N in ways:    \n",
    "    val_accs.append(training_pipeline.test_oneshot(model, N, trials, \"val\", verbose=True))\n",
    "    train_accs.append(training_pipeline.test_oneshot(model, N, trials, \"train\", verbose=True))\n",
    "    nn_acc = test_nn_accuracy(N, trials)\n",
    "    nn_accs.append(nn_acc)\n",
    "    print (\"NN Accuracy = \", nn_acc)\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save accuracies on disk\n",
    "with open(os.path.join(project_path,\"accuracies.pickle\"), \"wb\") as f:\n",
    "    pickle.dump((val_accs,train_accs,nn_accs),f)\n",
    "    \n",
    "#Load accuraces from disk\n",
    "with open(os.path.join(project_path, \"accuracies.pickle\"), \"rb\") as f:\n",
    "    (val_accs, train_accs, nn_accs) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_images(X):\n",
    "    \"\"\"Concatenates a bunch of images into a big matrix for plotting purposes.\"\"\"\n",
    "    nc, h , w, _ = X.shape\n",
    "    X = X.reshape(nc, h, w)\n",
    "    n = np.ceil(np.sqrt(nc)).astype(\"int8\")\n",
    "    img = np.zeros((n*h,n*w))\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for example in range(nc):\n",
    "        img[x*h:(x+1)*h,y*w:(y+1)*w] = X[example]\n",
    "        y += 1\n",
    "        if y >= n:\n",
    "            y = 0\n",
    "            x += 1\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_oneshot_task(pairs):\n",
    "    fig,(ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,15))\n",
    "    ax1.matshow(pairs[0][0].reshape(30,2), cmap='gray')\n",
    "    img = concat_images(pairs[1])\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax2.matshow(img,cmap='gray')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of concat image visualization\n",
    "pairs, targets = training_pipeline.make_oneshot_task(1,pos_train,neg_train,spoof_ground_truth.spoof_step1_truth1)\n",
    "plot_oneshot_task(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1)\n",
    "ax.plot(ways, val_accs, \"m\", label=\"Siamese(val set)\")\n",
    "ax.plot(ways, train_accs, \"y\", label=\"Siamese(train set)\")\n",
    "plt.plot(ways, nn_accs, label=\"Nearest neighbour\")\n",
    "\n",
    "ax.plot(ways, 100.0/ways, \"g\", label=\"Random guessing\")\n",
    "plt.xlabel(\"Number of possible classes in one-shot tasks\")\n",
    "plt.ylabel(\"% Accuracy\")\n",
    "plt.title(\"Omiglot One-Shot Learning Performance of a Siamese Network\")\n",
    "box = ax.get_position()\n",
    "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "#inputs,targets = make_oneshot_task(20, \"val\", 'Oriya')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def _normalize_img(img, label):\n",
    "    img = tf.cast(img, tf.float32) / 255.\n",
    "    return (img, label)\n",
    "\n",
    "train_dataset, test_dataset = tfds.load(name=\"mnist\", split=['train', 'test'], as_supervised=True)\n",
    "\n",
    "# Build your input pipelines\n",
    "train_dataset = train_dataset.shuffle(1024).batch(32)\n",
    "train_dataset = train_dataset.map(_normalize_img)\n",
    "\n",
    "test_dataset = test_dataset.batch(32)\n",
    "test_dataset = test_dataset.map(_normalize_img)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28,1)),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation=None), # No activation on final dense layer\n",
    "    tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) # L2 normalize embeddings\n",
    "\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tfa.losses.TripletSemiHardLoss())\n",
    "\n",
    "# Train the network\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=5)\n",
    "\n",
    "# Evaluate the network\n",
    "results = model.predict(test_dataset)\n",
    "\n",
    "# Save test embeddings for visualization in projector\n",
    "np.savetxt(\"vecs.tsv\", results, delimiter='\\t')\n",
    "\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for img, labels in tfds.as_numpy(test_dataset):\n",
    "    [out_m.write(str(x) + \"\\n\") for x in labels]\n",
    "out_m.close()\n",
    "\n",
    "\n",
    "\n",
    "from google.colab import files\n",
    "files.download('vecs.tsv')\n",
    "files.download('meta.tsv')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasons for Choices\n",
    "Normalisation - https://stats.stackexchange.com/questions/7757/data-normalization-and-standardization-in-neural-networks\n",
    "https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029\n",
    "https://towardsdatascience.com/why-data-should-be-normalized-before-training-a-neural-network-c626b7f66c7d\n",
    "https://www.jeremyjordan.me/batch-normalization/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
